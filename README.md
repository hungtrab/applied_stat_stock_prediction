# Stock Price Prediction for Applied Statistics

**Presentation & Live Demo:**

*   **View our Presentation Slides:** [Presentation Slide](https://docs.google.com/presentation/d/1whhVe0p_S_Xeq7z5wc21U4zkMSajVBx_/edit?usp=sharing&ouid=104000728711720335594&rtpof=true&sd=true)
*   **Experience the Live Application:** [Web Application](https://stockbook.streamlit.app/)

---

## 1. Team Members

| Name                  | Student ID   |
| :-------------------- | :----------- |
| Chu Anh Đức  | 20230081        |
| Vũ Thường Tín  | 20230091        |
| Trần Quang Hưng    | 20235502        |
| Đỗ Đăng Vũ  | 20235578        |
| Phan Đình Trung  | 20230093        |

---

## 2. Project Introduction

This project aims to apply various Machine Learning (ML) and Deep Learning (DL) algorithms to the task of stock price prediction. We explore and implement models to forecast future stock movements, focusing on both regression (predicting price values) and classification.

**Models Implemented:**

*   **Regression:**
    *   XGBoost
    *   Random Forest Regressor
    *   Gated Recurrent Unit (GRU)
    *   Long Short-Term Memory (LSTM)
*   **Classification:**
    *   K-Nearest Neighbors (KNN)

The primary tickers analyzed are `^GSPC` (S&P 500 Index) and `IBM`. Data is primarily sourced using the `yfinance` library.

---

## 3. Prediction Visualizations

Below are examples of prediction plots generated by our models. These visualizations help in comparing the predicted values against the actual historical prices.

![S&P 500 Prediction Plot](link_or_path_to_gspc_lstm_plot.png)

![IBM XGBoost Prediction Plot](link_or_path_to_ibm_xgboost_plot.png)



---

## 4. Project Setup and Running Instructions

This project is designed to be run using Python locally or via Docker and Docker Compose for a containerized environment.

**Prerequisites:**

*   Git
*   Python 3.12 (or a compatible 3.x version as per `requirements.txt`)
*   `pip` and `venv` (for local Python setup)
*   Docker (for containerized setup)
*   Docker Compose (for containerized setup)

### Method 1: Running Locally (via GitHub Source)

1.  **Clone the Repository:**
    ```bash
    git clone https://github.com/hungtrab/applied_stat_stock_prediction.git
    cd applied_stat_stock_prediction
    ```

2.  **Set up Python Virtual Environment and Install Dependencies:**
    ```bash
    python -m venv venv
    # Activate the virtual environment
    # On Windows:
    # .\venv\Scripts\activate
    # On macOS/Linux:
    # source venv/bin/activate
    pip install -r requirements.txt
    ```

3.  **Initial Data Seeding (Run only once or when refreshing all data):**
    Run the following commands sequentially:
    ```bash
    python -m app.data_ingestion
    python -m app.data_processing
    python -m app.seeder
    ```

4.  **Run the Application Components:**
    You will need three separate terminal windows/tabs. Make sure your virtual environment is activated in each.

    *   **Terminal 1: API Server (FastAPI with Uvicorn)**
        ```bash
        python -m uvicorn app.api_server:app --host localhost --port 8000 --reload
        ```
    *   **Terminal 2: Main Worker (APScheduler)**
        ```bash
        python -m app.main_worker
        ```
    *   **Terminal 3: Streamlit UI**
        ```bash
        streamlit run ui/app.py --server.port 8501 --server.address localhost
        ```

5.  **Access the User Interface:**
    Open your web browser and navigate to: `http://localhost:8501`

6.  **To Stop the Application:**
    Press `Ctrl+C` in each of the three terminals.

### Method 2: Running with Docker and Docker Compose

This method uses pre-built Docker images and orchestrates the services using Docker Compose.

1.  **Pull the Docker Image from Docker Hub:**
    ```bash
    docker pull hungtrab/stock_prediction:latest
    ```

2.  **Prepare `docker-compose.yml` file:**
    *   Ensure you have the `docker-compose.yml` file (provided in this repository) in your project directory.


3.  **Initial Data Seeding (Run only once for new volumes, or when refreshing all data):**
    ```bash
    # Fetch raw data and save to CSVs inside the data volume
    docker-compose run --rm scheduler_worker python -m application.data_ingestion

    # Process raw data into processed CSVs
    docker-compose run --rm scheduler_worker python -m application.data_processing

    # Seed the database
    docker-compose run --rm scheduler_worker python -m application.seeder
    ```

4.  **Run the Application with Docker Compose:**
    From the directory containing your `docker-compose.yml` file:
    ```bash
    docker-compose up -d
    ```
    The `-d` flag runs the containers in detached mode (in the background).

5.  **Access the User Interface:**
    Open your web browser and navigate to: `http://localhost:8501`

6.  **To Stop the Application:**
    From the directory containing your `docker-compose.yml` file:
    ```bash
    docker-compose down
    ```
    To also remove the named volumes (and delete data like the database):
    ```bash
    docker-compose down -v
    ```

---